{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "one_step_sampled_softmax_example.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "1g8EMYjBxT1LTwP3Z6QKm5b-UnoeIYBO_",
          "timestamp": 1524304953038
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "dIjM1kw644XN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## One-step sampled-softmax"
      ]
    },
    {
      "metadata": {
        "id": "u9dD-ts_5BEz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Example of sampled softmax for learning from large-scale vocabulary Seq-2Seq tasks (e.g. text generation in which number of class labels is vocabulary size in document corpus)"
      ]
    },
    {
      "metadata": {
        "id": "vUeBIgKnNOGu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "134d9264-c3dc-488f-dcd8-287e63dad749",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524337674893,
          "user_tz": -120,
          "elapsed": 2224,
          "user": {
            "displayName": "Iftitahu Nimah",
            "photoUrl": "//lh5.googleusercontent.com/-2H8SGwD_zvc/AAAAAAAAAAI/AAAAAAAAPGY/qh04HjJj8ZQ/s50-c-k-no/photo.jpg",
            "userId": "111575679600498524578"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages\r\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras)\n",
            "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
            "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6UuikuyhNhdL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8eac1ba8-255f-422e-aa7f-6cd4b0107a26",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524337676436,
          "user_tz": -120,
          "elapsed": 1483,
          "user": {
            "displayName": "Iftitahu Nimah",
            "photoUrl": "//lh5.googleusercontent.com/-2H8SGwD_zvc/AAAAAAAAAAI/AAAAAAAAPGY/qh04HjJj8ZQ/s50-c-k-no/photo.jpg",
            "userId": "111575679600498524578"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "#__author__ = \"@inimah\"\n",
        "#__date__ = \"20.04.2018\"\n",
        "\n",
        "from keras.layers import Dense, Lambda, Reshape\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Input, Concatenate, Masking, Layer, Flatten\n",
        "from keras.layers import LSTM\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "import tensorflow as tf"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "X8F2IW3OW-Zv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define class for creating custom layer to sample from class label distribution and operate sampled softmax loss"
      ]
    },
    {
      "metadata": {
        "id": "2bSStrvyNM5M",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "class SamplingLayer(Layer):\n",
        "    def __init__(self, num_sampled, num_classes, mode, **kwargs):\n",
        "        self.num_sampled = num_sampled\n",
        "        self.num_classes = num_classes\n",
        "        self.mode = mode\n",
        "        super(SamplingLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        dense_shape, classes_shape = input_shape\n",
        "        self.kernel = self.add_weight(name='kernel',\n",
        "                                      shape=(self.num_classes, dense_shape[1]),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)\n",
        "        self.bias = self.add_weight(name='bias',\n",
        "                                      shape=(self.num_classes,),\n",
        "                                      initializer='uniform',\n",
        "                                      trainable=True)  \n",
        "\n",
        "        super(SamplingLayer, self).build(input_shape)  \n",
        "\n",
        "    def call(self, inputs_and_labels):\n",
        "        inputs, labels = inputs_and_labels\n",
        "        if self.mode == \"train\":\n",
        "            loss = tf.nn.sampled_softmax_loss(\n",
        "                weights=self.kernel,\n",
        "                biases=self.bias,\n",
        "                labels=labels,\n",
        "                inputs=inputs,\n",
        "                num_sampled=self.num_sampled,\n",
        "                num_classes=self.num_classes,\n",
        "                num_true=1)\n",
        "\n",
        "        elif self.mode == \"eval\":\n",
        "            logits = tf.matmul(inputs, tf.transpose(self.kernel))\n",
        "            logits = tf.nn.bias_add(logits, self.bias)\n",
        "            labels_one_hot = tf.one_hot(labels, self.num_classes)\n",
        "            loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "                labels=labels_one_hot,\n",
        "                logits=logits)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        dense_shape, classes_shape = input_shape\n",
        "        return (dense_shape[0], self.num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UfroDlBQwD23",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create model with one-step softmax prediction"
      ]
    },
    {
      "metadata": {
        "id": "tPYzU-U_NYhh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "ea52ef5c-9f53-497c-eb98-d4ac358e232d",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524337679957,
          "user_tz": -120,
          "elapsed": 1822,
          "user": {
            "displayName": "Iftitahu Nimah",
            "photoUrl": "//lh5.googleusercontent.com/-2H8SGwD_zvc/AAAAAAAAAAI/AAAAAAAAPGY/qh04HjJj8ZQ/s50-c-k-no/photo.jpg",
            "userId": "111575679600498524578"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "vocab_size = 30\n",
        "n_features = 3\n",
        "batch_size = 10\n",
        "sequence_len = 5\n",
        "embedding_size = 10\n",
        "\n",
        "# Inputs\n",
        "x_input = Input((sequence_len,), dtype='int32', name='in_seq')\n",
        "aux_features = Input((sequence_len, n_features,), dtype='float', name='in_aux')\n",
        "labels = Input((sequence_len,1), dtype='int32', name='labels_') \n",
        "\n",
        "# masking and projecting to embedding layer\n",
        "masked_x_input = Masking(mask_value=0, name='masking_layer')(x_input)\n",
        "in_embed = Embedding(output_dim=embedding_size, input_dim=vocab_size, input_length=sequence_len, name='embedding_layer')(masked_x_input)\n",
        "\n",
        "in_merged = Concatenate(name='merged_inputs')([in_embed, aux_features])\n",
        "\n",
        "# LSTM layer with return sequences\n",
        "lstm_layer = LSTM(256, return_sequences=True, name='lstm_layer')(in_merged)\n",
        "\n",
        "losses = []\n",
        "#loss_evals = [] # use this for validation (uncomment)\n",
        "for t in range(sequence_len):\n",
        "  lstm_t = Lambda(lambda x: lstm_layer[:,t,:], name='lstm-%s'%t)(lstm_layer)\n",
        "  label_t = Lambda(lambda x: labels[:,t,:], name='label-%s'%t)(labels)\n",
        "  loss = SamplingLayer(10, vocab_size, mode='train', name='sampled_layer-%s'%t)([lstm_t, label_t])\n",
        "  #eval_loss = SamplingLayer(10, vocab_size, mode='eval', name='eval_layer-%s'%t)([lstm_t, label_t]) # use this for validation (uncomment)\n",
        "  losses.append(loss)\n",
        "  #loss_evals.append(eval_loss) # use this for validation (uncomment)\n",
        "#losses_ = losses + loss_evals # use this for validation (uncomment)\n",
        "\n",
        "#model = Model(inputs=[x_input, aux_features, labels], outputs=losses_) (uncomment)\n",
        "model = Model(inputs=[x_input, aux_features, labels], outputs=losses)\n",
        "model.compile(loss=lambda y_true, loss: loss, optimizer='Adam')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:1344: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oE3PyHLyhvuO",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "0508a946-7003-44cd-f1cf-a64a055a1535",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524337681533,
          "user_tz": -120,
          "elapsed": 492,
          "user": {
            "displayName": "Iftitahu Nimah",
            "photoUrl": "//lh5.googleusercontent.com/-2H8SGwD_zvc/AAAAAAAAAAI/AAAAAAAAPGY/qh04HjJj8ZQ/s50-c-k-no/photo.jpg",
            "userId": "111575679600498524578"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "in_seq (InputLayer)             (None, 5)            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "masking_layer (Masking)         (None, 5)            0           in_seq[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "embedding_layer (Embedding)     (None, 5, 10)        300         masking_layer[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "in_aux (InputLayer)             (None, 5, 3)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "merged_inputs (Concatenate)     (None, 5, 13)        0           embedding_layer[0][0]            \n",
            "                                                                 in_aux[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_layer (LSTM)               (None, 5, 256)       276480      merged_inputs[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "labels_ (InputLayer)            (None, 5, 1)         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm-0 (Lambda)                 (None, 256)          0           lstm_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "label-0 (Lambda)                (None, 1)            0           labels_[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm-1 (Lambda)                 (None, 256)          0           lstm_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "label-1 (Lambda)                (None, 1)            0           labels_[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm-2 (Lambda)                 (None, 256)          0           lstm_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "label-2 (Lambda)                (None, 1)            0           labels_[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm-3 (Lambda)                 (None, 256)          0           lstm_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "label-3 (Lambda)                (None, 1)            0           labels_[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm-4 (Lambda)                 (None, 256)          0           lstm_layer[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "label-4 (Lambda)                (None, 1)            0           labels_[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sampled_layer-0 (SamplingLayer) (None, 30)           7710        lstm-0[0][0]                     \n",
            "                                                                 label-0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sampled_layer-1 (SamplingLayer) (None, 30)           7710        lstm-1[0][0]                     \n",
            "                                                                 label-1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sampled_layer-2 (SamplingLayer) (None, 30)           7710        lstm-2[0][0]                     \n",
            "                                                                 label-2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sampled_layer-3 (SamplingLayer) (None, 30)           7710        lstm-3[0][0]                     \n",
            "                                                                 label-3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sampled_layer-4 (SamplingLayer) (None, 30)           7710        lstm-4[0][0]                     \n",
            "                                                                 label-4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 315,330\n",
            "Trainable params: 315,330\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PL8Iul_pOJFh",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# generate random labels with size (batch_size, sequence_length)\n",
        "y = np.random.randint(vocab_size, size=(batch_size, sequence_len))\n",
        "# transform y_true labels to one hot categorial encoding\n",
        "y_one_hot = to_categorical(y, vocab_size)\n",
        "\n",
        "# reshape y to 3D dimension (batch_size, sequence_length, 1)\n",
        "y = y.reshape((y.shape[0], y.shape[1], 1))\n",
        "\n",
        "x_in = np.array([np.random.choice(np.arange(vocab_size), sequence_len, replace=True) for _ in range(batch_size)])\n",
        "x_features = np.random.rand(batch_size, sequence_len, n_features)\n",
        "\n",
        "outputs = list(y_one_hot.swapaxes(0,1))\n",
        "\n",
        "## uncomment these to validate sampled softmax approach\n",
        "# in this example, we duplicate output, each for sampling layer (mode='train') and eval / validation layer (mode='eval')\n",
        "# for real use, one part can be training set labels, second part can be validation labels\n",
        "# outs = outputs + outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SwbNU_f7xVfm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "c6887628-5ce9-4528-dffc-1bc1917a8e86",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1524337686799,
          "user_tz": -120,
          "elapsed": 3980,
          "user": {
            "displayName": "Iftitahu Nimah",
            "photoUrl": "//lh5.googleusercontent.com/-2H8SGwD_zvc/AAAAAAAAAAI/AAAAAAAAPGY/qh04HjJj8ZQ/s50-c-k-no/photo.jpg",
            "userId": "111575679600498524578"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# model.fit([x_in, x_features, y], outs, epochs=10) ## uncomment these to validate sampled softmax approach\n",
        "\n",
        "model.fit([x_in, x_features, y], outputs, epochs=10)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 1s 127ms/step - loss: 10.6637 - sampled_layer-0_loss: 2.3453 - sampled_layer-1_loss: 2.2285 - sampled_layer-2_loss: 2.1036 - sampled_layer-3_loss: 2.0598 - sampled_layer-4_loss: 1.9265\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 11.5569 - sampled_layer-0_loss: 2.2327 - sampled_layer-1_loss: 2.4230 - sampled_layer-2_loss: 2.5658 - sampled_layer-3_loss: 2.1560 - sampled_layer-4_loss: 2.1795\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 11.0799 - sampled_layer-0_loss: 2.3356 - sampled_layer-1_loss: 2.2346 - sampled_layer-2_loss: 2.4137 - sampled_layer-3_loss: 2.2126 - sampled_layer-4_loss: 1.8834\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 10.1655 - sampled_layer-0_loss: 1.9289 - sampled_layer-1_loss: 2.1252 - sampled_layer-2_loss: 2.1565 - sampled_layer-3_loss: 2.0357 - sampled_layer-4_loss: 1.9191\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 10.7522 - sampled_layer-0_loss: 1.9391 - sampled_layer-1_loss: 2.2295 - sampled_layer-2_loss: 2.3216 - sampled_layer-3_loss: 2.3155 - sampled_layer-4_loss: 1.9465\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 10.8622 - sampled_layer-0_loss: 2.4013 - sampled_layer-1_loss: 2.0932 - sampled_layer-2_loss: 2.2217 - sampled_layer-3_loss: 2.2476 - sampled_layer-4_loss: 1.8985\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 11.0063 - sampled_layer-0_loss: 2.2979 - sampled_layer-1_loss: 2.3589 - sampled_layer-2_loss: 2.2759 - sampled_layer-3_loss: 2.0750 - sampled_layer-4_loss: 1.9987\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 10.7078 - sampled_layer-0_loss: 2.3783 - sampled_layer-1_loss: 2.0070 - sampled_layer-2_loss: 2.2428 - sampled_layer-3_loss: 2.1966 - sampled_layer-4_loss: 1.8831\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 10.6091 - sampled_layer-0_loss: 2.2140 - sampled_layer-1_loss: 2.3126 - sampled_layer-2_loss: 2.1329 - sampled_layer-3_loss: 2.1443 - sampled_layer-4_loss: 1.8053\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 10.8024 - sampled_layer-0_loss: 2.6642 - sampled_layer-1_loss: 2.1538 - sampled_layer-2_loss: 2.0217 - sampled_layer-3_loss: 2.2192 - sampled_layer-4_loss: 1.7435\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3ecbab85c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}